["Port to keras", "As per https://github.com/tensorflow/community/pull/252.    Old PR for C++ kernels: https://github.com/tensorflow/tensorflow/pull/33945    cc @seanpmorgan @alextp", "Thanks @WindQAQ!     Just wanted to add a note that per discussion there is an upcoming RFC for adding custom op kernels alongside python composite ops. Once that is made public a future PR can add those since they've been requested in the migration RFC.    @alextp Is there a need for an advanced activation layer? What is the criteria for those?  https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/keras/layers/advanced_activations.py", "run pylint", "Update golden api", "Update golden api", "DISMISSED", "COMMENTED Can we add doc tests with examples showing how to use this? https://www.tensorflow.org/community/contribute/docs_ref    Also  it would be helpful to add a brief description of GELUs in addition to the existing paper link.", "COMMENTED Can we add doc tests with examples showing how to use this? https://www.tensorflow.org/community/contribute/docs_ref    Also  it would be helpful to add a brief description of GELUs in addition to the existing paper link.", "CHANGES_REQUESTED for @tensorflow/api-owners", "COMMENTED In general  we prefer to only export to v2  as TFv1 is no longer released. Can you update this + the export below to have an empty list for v1? Eg:    ```  @keras_export('keras.activations.gelu'  v1=[])  ```", "COMMENTED Hi @karmel  does this also apply to `tf.nn.gelu`  like  ```python  @tf_export(\"nn.gelu\"  v1=[])  ```", "Hi @karmel  do we also need to put `GELU` into [advanced_activations](https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/keras/layers/advanced_activations.py) in order for `Layer` subclass? Thank you!", "Add example and more doc", "Fix wrong module", "Export only v2 api", "Run pylint", "When the C++ kernels are added it might be useful to have approximate=False by default.  https://github.com/pytorch/pytorch/issues/39853#issuecomment-658806898  It doesn't seem like the approximate version adds any speed over an optimized exact version.", "Merge branch 'master' of https://github.com/WindQAQ/tensorflow into migrate-python-gelu", "DISMISSED for @tensorflow/api-owners     @WindQAQ - it is fine to skip making a layer in advanced_activations.py for now.    If  however  as the comment from @hendrycks suggests  we want approximate=False to be the default in the future  we should make that the default now  as we will not be able to change from True to False in the future.", "> for @tensorflow/api-owners  >   > @WindQAQ - it is fine to skip making a layer in advanced_activations.py for now.  >   > If  however  as the comment from @hendrycks suggests  we want approximate=False to be the default in the future  we should make that the default now  as we will not be able to change from True to False in the future.    Got it. Let alone the speed  two versions of gelu (approximate or not) produce different values. In my opinion  because the existing BERT-like models use the approximation version  if we change it to non-approximation by default  it will somehow loss some precision in the pretrained models.    https://colab.research.google.com/drive/1UK47XgQgAWzvG4PwCk1jNI8VAIm0AQEL?usp=sharing", "> it will somehow loss some precision in the pretrained models.    I think [BERT](https://github.com/huggingface/transformers/blob/82601f4c1a5c3edb680593bdd9b54abd5846cfa7/src/transformers/activations.py#L16)  RoBERTa  and many (but not all) recent models have used the exact version. I also believe the exact version is more numerically stable. It should be noted that PyTorch doesn't even offer the approximate version and only has the exact version. As an author of the GELU paper  I think it would be good if we all started using the exact version since the approximations do not seem to provide speed anymore.", "Change approximate default to False", "Merge branch 'master' of https://github.com/WindQAQ/tensorflow into migrate-python-gelu", "Update tests", "Hi @karmel   I change the default value :-) Please review again. Thank you!", "> > it will somehow loss some precision in the pretrained models.  >   > I think [BERT](https://github.com/huggingface/transformers/blob/82601f4c1a5c3edb680593bdd9b54abd5846cfa7/src/transformers/activations.py#L16)  RoBERTa  and many recent models have used the exact version. I also believe the exact version is more numerically stable. It should be noted that PyTorch doesn't even offer the approximate version and only has the exact version. As an author of the GELU paper  I think it would be good if we all started using the exact version since the approximations do not seem to provide speed anymore.    We have been used the approximate versions for many applications and many follow-up papers are using the approximate version  which at least performs well on TPU.", "> We have been used the approximate versions for many applications and many follow-up papers are using the approximate version  which at least performs well on TPU.    The current commit keeps the approximate form as an option.", "Hi @alextp @karmel  any suggestion on default value of the argument `approximate`? Thank you.", "approximate=False should be the default  On Mon  Jul 20  2020 at 9:07 AM Tzu-Wei Sung <notifications@github.com> wrote:  > Hi @alextp <https://github.com/alextp> @karmel <https://github.com/karmel>  > any suggestion on default value of the argument approximate? Thank you. > > \u2014 > You are receiving this because you were mentioned. > Reply to this email directly  view it on GitHub > <https://github.com/tensorflow/tensorflow/pull/41178#issuecomment-661134839>  > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AAABHRO3JQLVVVIHCSES5P3R4RTSLANCNFSM4OUA24CA> > . >   --   - Alex", "APPROVED", "Close event", "Merge event", "@WindQAQ @seanpmorgan @penpornk     I am confused if the RFC for C++ kernel implementation is created? Any updates? I understand the Python wrapper function is available now and merged in the master    Here is the discussion link:  https://github.com/tensorflow/tensorflow/pull/41178#issuecomment-655231498", "Waiting for an upcoming RFC on being able to easily support custom-ops with python op fallbacks. cc @alextp to see if there is any update on that?", "@seanpmorgan Sorry for the delay! @allenlavoie told me custom device support could be used for this: https://github.com/tensorflow/tensorflow/blob/84967b39fa98d27f5984648f9ec47a159206cfda/tensorflow/c/eager/c_api_experimental.h#L477-L517    In the long run (i.e.  not now)  @allenlavoie also has an [RFC on front-end op handlers](https://github.com/tensorflow/community/pull/275) that could help as well.", "One challenge with the custom device approach is that they're currently mutually exclusive with physical devices (an op is either on a physical device or a custom device). So if we're thinking of something that filters through every eager operation and is turned on globally then the custom device approach seems challenging for anything other than prototyping.    The op handler RFC proposes an updated version of that mechanism which isn't mutually exclusive with physical devices  and there I could see a global composite op lowering handler working. But even once that's implemented  it being useful is blocked on Python migrating to the eager/graph agnostic C API (see \"risks\" in the proposal). I don't expect it to be implemented quickly.", "Talked to Alex and he actually meant `tf.composite` by @liufengdb  so here it is  as another alternative:  [Code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/tfr)  [Examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/tfr/examples)    @liufengdb plans to add some README and documentation soon.", ""]